import numpy as np
from gymnasium import utils
from gymnasium.envs.mujoco import MujocoEnv
from gymnasium.spaces import Box

class ReacherEnv(MujocoEnv, utils.EzPickle):
    metadata = {"render_modes": ["human", "rgb_array"]}

    def __init__(self, xml_file="reacher.xml", frame_skip=2, **kwargs):
        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)
        
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„ Boxï¼Œä»…ç”¨äºé€šè¿‡ MujocoEnv çš„åŸºç±»æ ¡éªŒ
        # å®é™…çš„ Dict ç©ºé—´ç”± Wrapper å®šä¹‰
        dummy_space = Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)
        
        MujocoEnv.__init__(self, xml_file, frame_skip, observation_space=dummy_space, **kwargs)

        self.max_joints = 10
        # âœ… è®©ç¯å¢ƒæ‰¿è®¤ 10 ç»´åŠ¨ä½œç©ºé—´ï¼ŒåŒ¹é…ç½‘ç»œè¾“å‡º
        self.action_space = Box(low=-1.0, high=1.0, shape=(self.max_joints,), dtype=np.float32)
        
        self.n_joints = self.model.nu 
        self.success_threshold = 0.10 

    def step(self, action):
        actual_action = action[:self.n_joints]
        self.do_simulation(actual_action, self.frame_skip)
        
        observation = self._get_obs()
        reward, reward_info = self._get_rew(actual_action)
        
        dist = self._get_dist()
        
        # âœ… æ ¸å¿ƒæ”¹å˜ 1ï¼šæ°¸è¿œä¸æå‰ terminatedï¼ä¸ç®¡ç¢°æ²¡ç¢°åˆ°ï¼Œå¿…é¡»å¹²æ»¡ 50 å¸§ã€‚
        terminated = False 
        
        # âœ… æ ¸å¿ƒæ”¹å˜ 2ï¼šâ€œæ‰“å¡å·¥èµ„â€å˜æˆâ€œé©»ç•™æ—¶è–ªâ€
        # åªè¦æ‰‹å°–åœ¨çº¢çƒé‡Œï¼Œã€æ¯ä¸€å¸§ã€‘éƒ½ç»™ +10 åˆ†ï¼
        # å¦‚æœå®ƒç¬¬ä¸€ç§’å°±åˆ°äº†å¹¶é»ä½ï¼Œä¸€å±€èƒ½æ‹¿å‡ ç™¾åˆ†çš„æš´åˆ©ï¼
        if dist < self.success_threshold:
            reward += 10.0 
            
        if self.render_mode == "human": self.render()
        return observation, reward, terminated, False, reward_info

    def _get_dist(self):
        return np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target")[:2])

    def _get_rew(self, action):
        dist = self._get_dist()
        
        # è·ç¦»è¶Šè¿œï¼Œä¾ç„¶ä¼šæœ‰å°é¢æ‰£åˆ†ï¼Œç”¨æ¥æŒ‡å¼•æ–¹å‘
        reward_dist = -dist 
        
        # è¿›åº¦å¥–åŠ±ä¿ç•™ï¼Œè®©å®ƒåœ¨æ²¡ç¢°åˆ°çƒä¹‹å‰èƒ½é¡ºç€æ°”å‘³æ‰¾è¿‡å»
        reward_progress = 0.0
        if self.prev_dist is not None:
            reward_progress = (self.prev_dist - dist) * 10.0 
            
        reward_ctrl = -0.01 * np.square(action).sum()

        # ğŸš¨ åˆ é™¤äº† step_penaltyã€‚ä¸éœ€è¦çš®é­äº†ï¼Œå‰æ–¹çš„â€œæ¯å¸§ +10 åˆ†â€å°±æ˜¯æœ€å¼ºç£é“ã€‚

        self.prev_dist = dist
        return reward_dist + reward_progress + reward_ctrl, {"dist": dist}

    def reset_model(self):
        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
        while True:
            self.goal = self.np_random.uniform(low=-0.2, high=0.2, size=2)
            if np.linalg.norm(self.goal) < 0.2: break
        qpos[-2:] = self.goal
        qvel = self.init_qvel + self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
        qvel[-2:] = 0
        self.set_state(qpos, qvel)
        
        # âœ… è‡´å‘½ Bug ä¿®å¤ï¼šæ¯å±€å¼€å§‹å‰ï¼Œå¿…é¡»æŠŠ prev_dist è®¾ä¸ºå½“å‰çš„ç»å¯¹åˆå§‹è·ç¦»ï¼
        # å¦åˆ™è¿›åº¦å¥–åŠ±ä¼šå‘ç”Ÿæå…¶ç¦»è°±çš„â€œè·¨å±€æ±¡æŸ“â€ã€‚
        self.prev_dist = self._get_dist()
        
        return self._get_obs()

    def _get_obs(self):
        return {
            "qpos": self.data.qpos.flat[:self.n_joints].copy(),
            "qvel": self.data.qvel.flat[:self.n_joints].copy(),
            "target": self.get_body_com("target")[:2].copy(),
            "fingertip": self.get_body_com("fingertip")[:2].copy(),
        }