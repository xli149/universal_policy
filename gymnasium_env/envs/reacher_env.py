import numpy as np
from gymnasium import utils
from gymnasium.envs.mujoco import MujocoEnv
from gymnasium.spaces import Box

class ReacherEnv(MujocoEnv, utils.EzPickle):
    metadata = {"render_modes": ["human", "rgb_array"]}

    def __init__(self, xml_file="reacher.xml", frame_skip=2, **kwargs):
        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)
        
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„ Boxï¼Œä»…ç”¨äºé€šè¿‡ MujocoEnv çš„åŸºç±»æ ¡éªŒ
        dummy_space = Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float64)
        MujocoEnv.__init__(self, xml_file, frame_skip, observation_space=dummy_space, **kwargs)

        self.max_joints = 10
        # âœ… è®©ç¯å¢ƒæ‰¿è®¤ 10 ç»´åŠ¨ä½œç©ºé—´ï¼ŒåŒ¹é…ç½‘ç»œè¾“å‡º
        self.action_space = Box(low=-1.0, high=1.0, shape=(self.max_joints,), dtype=np.float32)
        
        self.n_joints = self.model.nu 
        self.success_threshold = 0.10 

    def step(self, action):
        actual_action = action[:self.n_joints]
        self.do_simulation(actual_action, self.frame_skip)
        
        observation = self._get_obs()
        reward, reward_info = self._get_rew(actual_action)
        
        # âœ… æ ¸å¿ƒæ”¹å˜ï¼šå½»åº•å‰”é™¤ "+10 æ‚¬åœå¥–é‡‘"ï¼
        # å®˜æ–¹ v5 æ ¹æœ¬æ²¡æœ‰æˆåŠŸå¥–é‡‘ï¼Œåªæœ‰â€œæ²¡ç¢°åˆ°çƒæ—¶çš„æ‰£åˆ†â€ã€‚
        # å®ƒå¿…é¡»ä¸ºäº†ã€å°‘æ‰£åˆ†ã€‘è€Œæ‹¼å‘½é£å‘çº¢çƒï¼Œå¹¶ä¸ºäº†ã€ä¸æ‰£åŠ¨ä½œåˆ†ã€‘è€Œå®‰é™åœä¸‹ã€‚
        
        if self.render_mode == "human": self.render()
        
        # æ°¸è¿œä¸æå‰ terminatedï¼ä¸ç®¡ç¢°æ²¡ç¢°åˆ°ï¼Œå¿…é¡»å¹²æ»¡ 50 å¸§ã€‚
        return observation, reward, False, False, reward_info

    def _get_dist(self):
        return np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target")[:2])

    def _get_rew(self, action):
        dist = self._get_dist()
        
        # 1. è·ç¦»æƒ©ç½šï¼ˆæœ€çº¯ç²¹çš„ç‰©ç†æŒ‡å¼•ï¼‰
        reward_dist = -dist 
        
        # ğŸš¨ æ ¸å¿ƒæ”¹å˜ï¼šå‰”é™¤â€œè¿›åº¦å¥–åŠ±â€
        # è¿›åº¦å¥–åŠ±å®¹æ˜“å¼•å‘å±€éƒ¨æœ€ä¼˜ï¼ˆæ¥å›éœ‡è¡åˆ·åˆ†ï¼‰ï¼Œå®˜æ–¹ v5 ä¸éœ€è¦å®ƒã€‚
        
        # âœ… æ ¸å¿ƒæ”¹å˜ï¼šå°†åŠ¨ä½œæƒ©ç½šæ”¾å¤§ 10 å€ï¼ï¼ˆè§£å†³â€œç”µé£æ‰‡ç–¯ç‹‚è½¬åœˆâ€çš„å…ƒå‡¶ï¼‰
        # ä» -0.01 ä¿®æ”¹ä¸º -0.1ã€‚çè½¬åœˆä¼šå¸¦æ¥æå…¶æƒ¨é‡çš„æ‰£åˆ†ï¼
        reward_ctrl = -0.1 * np.square(action).sum()

        # æœ€ç»ˆå¥–åŠ±å°±æ˜¯æç®€çš„ç‰©ç†åé¦ˆ
        reward = reward_dist + reward_ctrl
        
        return reward, {"dist": dist, "reward_dist": reward_dist, "reward_ctrl": reward_ctrl}

    def reset_model(self):
        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
        while True:
            self.goal = self.np_random.uniform(low=-0.2, high=0.2, size=2)
            if np.linalg.norm(self.goal) < 0.2: break
        qpos[-2:] = self.goal
        qvel = self.init_qvel + self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
        qvel[-2:] = 0
        self.set_state(qpos, qvel)
        
        # ğŸš¨ å‰”é™¤äº† self.prev_distï¼Œå› ä¸ºä¸å†éœ€è¦è®¡ç®—è¿›åº¦äº†ã€‚
        return self._get_obs()

    def _get_obs(self):
        # åŸæ±åŸå‘³ä¿ç•™ï¼Œè¿™äº›æ•°æ®è¶³å¤Ÿ Wrapper æå–ç›¸å¯¹ä½ç½®äº†
        return {
            "qpos": self.data.qpos.flat[:self.n_joints].copy(),
            "qvel": self.data.qvel.flat[:self.n_joints].copy(),
            "target": self.get_body_com("target")[:2].copy(),
            "fingertip": self.get_body_com("fingertip")[:2].copy(),
        }